{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from metrics import Metrics\n",
    "import key_tools as tools\n",
    "\n",
    "# Some magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  lang                                               text\n",
       "0   es  Alemania vs Argentina, la tercera es la vencid...\n",
       "1   en  I have gained level 39 in The Tribez and Castl...\n",
       "2   pt          Finalmente é sexta, mas ainda tenho teste\n",
       "3   fr  \"#Marée ↗ #Mimizan 14/04/2016 UTC+2 Basse mer ...\n",
       "4   es              Que ganitas de poder ir al gym jupe.."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lang</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>es</td>\n      <td>Alemania vs Argentina, la tercera es la vencid...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>en</td>\n      <td>I have gained level 39 in The Tribez and Castl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pt</td>\n      <td>Finalmente é sexta, mas ainda tenho teste</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fr</td>\n      <td>\"#Marée ↗ #Mimizan 14/04/2016 UTC+2 Basse mer ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>es</td>\n      <td>Que ganitas de poder ir al gym jupe..</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def LoadData(filename):\n",
    "    data = pandas.read_csv(filename, header=None, sep='\\t', quoting=3)\n",
    "    data.columns = ['lang', 'text']\n",
    "    return data\n",
    "data = LoadData('../Data/train.tsv')\n",
    "val_data = LoadData('../Data/val.tsv')\n",
    "test_data = LoadData('../Data/test.tsv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary tables\n",
    "all_langs = data.lang.unique()\n",
    "lang2idx = dict(zip(all_langs, range(len(all_langs))))\n",
    "idx2lang = dict(zip(lang2idx.values(), lang2idx.keys()))\n",
    "\n",
    "counts = collections.Counter()\n",
    "for line in data.text:\n",
    "    counts.update(line)\n",
    "\n",
    "chars = set([c for c in counts if counts[c] >= 10])\n",
    "chars.update(['PAD', '<S>', '</S>', 'UNK'])\n",
    "char2idx = dict(zip(chars, range(len(chars))))\n",
    "idx2char = dict(zip(range(len(chars)), chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "size of vocabulary is 510\nnumber of unique characters 1393\noov rate is 0.0469%\n"
     ]
    }
   ],
   "source": [
    "# size of vocabulary\n",
    "print('size of vocabulary is {0}'.format(len(chars)))\n",
    "print('number of unique characters {0}'.format(len(counts)))\n",
    "\n",
    "total_chars = sum(counts.values())\n",
    "total_oovs = sum([counts[c] for c in counts if counts[c] < 10])\n",
    "print('oov rate is {0:.4f}%'.format(100.0 * total_oovs / total_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the perplexity is 34.11\n"
     ]
    }
   ],
   "source": [
    "# WARM_UP: Compute the perplexity of a unigram model\n",
    "train_counts = np.zeros(len(chars))\n",
    "val_counts = np.zeros(len(chars))\n",
    "\n",
    "for line in data.text:\n",
    "    for c in line:\n",
    "        idx = char2idx.get(c, char2idx['UNK'])\n",
    "        train_counts[idx] += 1.0\n",
    "    train_counts[char2idx['</S>']] += 1.0\n",
    "for line in val_data.text:\n",
    "    for c in line:\n",
    "        idx = char2idx.get(c, char2idx['UNK'])\n",
    "        val_counts[idx] += 1.0\n",
    "    val_counts[char2idx['</S>']] += 1.0\n",
    "train_counts[char2idx['PAD']] += 1\n",
    "train_counts = train_counts / train_counts.sum()\n",
    "train_counts[char2idx['<S>']] = 1.0  # this will be zeroed out later\n",
    "val_counts = val_counts / val_counts.sum()\n",
    "ppl = np.exp(-(val_counts * np.log(train_counts)).sum())\n",
    "print('the perplexity is {0:.2f}'.format(ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(char2idx, idx2char, lang2idx, idx2lang, data, save_file, max_seq_length=1000):\n",
    "    sequences = []\n",
    "    languages = []\n",
    "    for i in range(len(data.text)):\n",
    "        line = data.text[i]\n",
    "        if len(line) > max_seq_length:\n",
    "            continue\n",
    "        seq = [char2idx['<S>']]\n",
    "        for c in line:\n",
    "            idx = char2idx.get(c, char2idx['UNK'])\n",
    "            seq += [idx]\n",
    "        seq += [char2idx['</S>']]\n",
    "        sequences += [seq]\n",
    "        languages += [lang2idx[data.lang[i]]]\n",
    "\n",
    "    pickle.dump({'chars': sequences, 'langs': languages, 'ind2voc': idx2char, 'voc2ind':char2idx, 'ind2lang': idx2lang, 'lang2ind':lang2idx}, open(save_file, 'wb'))\n",
    "    \n",
    "prepare_data(char2idx, idx2char, lang2idx, idx2lang, data, 'chars_train.pkl')\n",
    "prepare_data(char2idx, idx2char, lang2idx, idx2lang, val_data, 'chars_val.pkl')\n",
    "prepare_data(char2idx, idx2char, lang2idx, idx2lang, test_data, 'chars_test.pkl')\n",
    "\n",
    "\n",
    "PAD = char2idx['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, lang_size=9, char_vec_size=12, lang_vec_size=2, hidden_size=50, PAD=0):\n",
    "        \n",
    "        super(MyRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lang_size = lang_size\n",
    "        self.char_vec_size = char_vec_size\n",
    "        self.lang_vec_size = lang_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.char_encoder = nn.Embedding(self.vocab_size, self.char_vec_size)\n",
    "        self.lang_encoder = nn.Embedding(self.lang_size, self.lang_vec_size)\n",
    "        # the current hidden size = char_vec_size\n",
    "        self.gru = nn.GRU(self.char_vec_size+self.lang_vec_size, self.hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.char_vec_size)\n",
    "        self.decoder = nn.Linear(self.char_vec_size, self.vocab_size)\n",
    "        \n",
    "        # This shares the encoder and decoder weights as described in lecture.\n",
    "        self.decoder.weight = self.char_encoder.weight\n",
    "        self.decoder.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "        weight = torch.ones(vocab_size)\n",
    "        # scores over PAD is not counted\n",
    "        weight[PAD] = 0\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "        self.crit = nn.NLLLoss(weight, size_average=False)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        emb = pack(torch.cat((self.char_encoder(input[0]), self.lang_encoder(input[1])), -1), input[2])\n",
    "        output, hidden_t = self.gru(emb, hidden)\n",
    "        output = unpack(output)[0]\n",
    "        output = F.tanh(self.linear(output))\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden_t\n",
    "\n",
    "    # Predefined loss function\n",
    "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
    "        prediction = prediction.view(-1, self.vocab_size)\n",
    "        prediction = self.sm(prediction)\n",
    "        loss_val = self.crit(prediction, label.view(-1))\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "FEATURE_SIZE = 15\n",
    "TEST_BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "data_train = tools.Dataset('chars_train.pkl', BATCH_SIZE, PAD)\n",
    "data_val = tools.Dataset('chars_val.pkl', TEST_BATCH_SIZE, PAD)\n",
    "data_test = tools.Dataset('chars_test.pkl', TEST_BATCH_SIZE, PAD)\n",
    "\n",
    "model = MyRNN(len(char2idx),PAD=PAD)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['batch_size', 'PAD', 'data', 'langs', 'numBatches'])\n3\ntensor([429, 112, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190,\n        190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190,\n        190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190,\n        190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190, 190,\n        190, 190, 190, 190, 190, 190, 190, 190])\ntorch.Size([141, 64])\n[141, 140, 139, 138, 131, 126, 118, 116, 115, 114, 114, 106, 99, 95, 87, 86, 85, 80, 76, 75, 74, 73, 71, 70, 69, 68, 68, 67, 66, 66, 63, 62, 60, 60, 60, 56, 55, 54, 45, 42, 42, 40, 38, 37, 37, 36, 35, 34, 34, 34, 30, 29, 29, 28, 26, 24, 24, 23, 21, 20, 17, 10, 8, 5]\ntensor([0, 0, 2, 0, 0, 1, 3, 1, 2, 0, 3, 4, 5, 3, 2, 1, 1, 0, 0, 1, 0, 1, 0, 4,\n        2, 2, 6, 0, 2, 1, 2, 2, 2, 1, 2, 0, 3, 1, 2, 2, 1, 3, 0, 2, 2, 0, 1, 0,\n        2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 7, 0, 2, 1])\ntensor([0, 0, 2, 0, 0, 1, 3, 1, 2, 0, 3, 4, 5, 3, 2, 1, 1, 0, 0, 1, 0, 1, 0, 4,\n        2, 2, 6, 0, 2, 1, 2, 2, 2, 1, 2, 0, 3, 1, 2, 2, 1, 3, 0, 2, 2, 0, 1, 0,\n        2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 7, 0, 2, 1])\n<S>Alemania vs Argentina, la tercera es la vencida: El Mundial de Brasil 2014 cierra este domingo con una reedici...</S>\n115\n"
     ]
    }
   ],
   "source": [
    "# playing around\n",
    "print(data_train.__dict__.keys())\n",
    "\n",
    "data1, label1, _ = data_train[0]\n",
    "print(len(data1))\n",
    "print(data1[0][139])\n",
    "print(data1[1].size())\n",
    "print(data1[2])\n",
    "\n",
    "print(data1[1][0])\n",
    "print(data1[1][20])\n",
    "\n",
    "tweet = ''\n",
    "for c in data_train.data[0]:\n",
    "    tweet += idx2char.get(c.item())\n",
    "print(tweet)\n",
    "print(len(data_train.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [336, 436, 174,  ..., 278, 139, 370],\n",
      "        [ 99, 429, 448,  ...,  13, 429, 429],\n",
      "        ...,\n",
      "        [328, 438, 124,  ..., 190, 190, 190],\n",
      "        [429, 112, 190,  ..., 190, 190, 190],\n",
      "        [ 56, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[0, 0, 2,  ..., 0, 2, 1],\n",
      "        [0, 0, 2,  ..., 0, 2, 1],\n",
      "        [0, 0, 2,  ..., 0, 2, 1],\n",
      "        ...,\n",
      "        [0, 0, 2,  ..., 0, 2, 1],\n",
      "        [0, 0, 2,  ..., 0, 2, 1],\n",
      "        [0, 0, 2,  ..., 0, 2, 1]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.3956,  0.8194, -0.1348,  ..., -0.5961,  0.6711,  0.2792],\n",
      "        [-0.3956,  0.8194, -0.1348,  ..., -0.5961,  0.6711,  0.2792],\n",
      "        [-0.3353,  0.6443, -0.2289,  ..., -0.4849,  0.5773,  0.0859],\n",
      "        ...,\n",
      "        [-0.3956,  0.8194, -0.1348,  ..., -0.5961,  0.6711,  0.2792],\n",
      "        [-0.3353,  0.6443, -0.2289,  ..., -0.4849,  0.5773,  0.0859],\n",
      "        [-0.3787,  0.4722, -0.1848,  ..., -0.4516,  0.5849, -0.0716]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [373, 448, 448,  ..., 364, 411, 378],\n",
      "        [303, 429, 169,  ...,   1, 429, 180],\n",
      "        ...,\n",
      "        [ 56, 112, 177,  ..., 190, 190, 190],\n",
      "        [ 56,   1, 177,  ..., 190, 190, 190],\n",
      "        [ 56, 177, 190,  ..., 190, 190, 190]])\n",
      "tensor([[0, 0, 7,  ..., 2, 1, 0],\n",
      "        [0, 0, 7,  ..., 2, 1, 0],\n",
      "        [0, 0, 7,  ..., 2, 1, 0],\n",
      "        ...,\n",
      "        [0, 0, 7,  ..., 2, 1, 0],\n",
      "        [0, 0, 7,  ..., 2, 1, 0],\n",
      "        [0, 0, 7,  ..., 2, 1, 0]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.3719,  0.8186, -0.1252,  ..., -0.5945,  0.6612,  0.2645],\n",
      "        [-0.3719,  0.8186, -0.1252,  ..., -0.5945,  0.6612,  0.2645],\n",
      "        [-0.5221,  0.5871,  0.0417,  ..., -0.6203,  0.7663,  0.0744],\n",
      "        ...,\n",
      "        [-0.3162,  0.6450, -0.2197,  ..., -0.4835,  0.5676,  0.0745],\n",
      "        [-0.3630,  0.4730, -0.1771,  ..., -0.4495,  0.5750, -0.0816],\n",
      "        [-0.3719,  0.8186, -0.1252,  ..., -0.5945,  0.6612,  0.2645]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [308, 373, 231,  ..., 139, 139, 124],\n",
      "        [292, 150, 429,  ..., 429,   9, 112],\n",
      "        ...,\n",
      "        [150, 124, 190,  ..., 190, 190, 190],\n",
      "        [177, 177, 190,  ..., 190, 190, 190],\n",
      "        [177, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[0, 1, 1,  ..., 2, 0, 1],\n",
      "        [0, 1, 1,  ..., 2, 0, 1],\n",
      "        [0, 1, 1,  ..., 2, 0, 1],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 2, 0, 1],\n",
      "        [0, 1, 1,  ..., 2, 0, 1],\n",
      "        [0, 1, 1,  ..., 2, 0, 1]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.3480,  0.8192, -0.1173,  ..., -0.5924,  0.6515,  0.2504],\n",
      "        [-0.3472,  0.4746, -0.1705,  ..., -0.4473,  0.5655, -0.0910],\n",
      "        [-0.3472,  0.4746, -0.1705,  ..., -0.4473,  0.5655, -0.0910],\n",
      "        ...,\n",
      "        [-0.2969,  0.6468, -0.2118,  ..., -0.4817,  0.5581,  0.0637],\n",
      "        [-0.3480,  0.8192, -0.1173,  ..., -0.5924,  0.6515,  0.2504],\n",
      "        [-0.3472,  0.4746, -0.1705,  ..., -0.4473,  0.5655, -0.0910]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [247, 448, 177,  ..., 297, 139, 278],\n",
      "        [176, 139, 150,  ..., 455, 169, 446],\n",
      "        ...,\n",
      "        [  0, 321, 190,  ..., 190, 190, 190],\n",
      "        [422, 190, 190,  ..., 190, 190, 190],\n",
      "        [247, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[2, 0, 1,  ..., 1, 1, 1],\n",
      "        [2, 0, 1,  ..., 1, 1, 1],\n",
      "        [2, 0, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [2, 0, 1,  ..., 1, 1, 1],\n",
      "        [2, 0, 1,  ..., 1, 1, 1],\n",
      "        [2, 0, 1,  ..., 1, 1, 1]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.2771,  0.6493, -0.2056,  ..., -0.4796,  0.5486,  0.0524],\n",
      "        [-0.3237,  0.8206, -0.1116,  ..., -0.5898,  0.6421,  0.2357],\n",
      "        [-0.3309,  0.4767, -0.1651,  ..., -0.4447,  0.5558, -0.1007],\n",
      "        ...,\n",
      "        [-0.3309,  0.4767, -0.1651,  ..., -0.4447,  0.5558, -0.1007],\n",
      "        [-0.3309,  0.4767, -0.1651,  ..., -0.4447,  0.5558, -0.1007],\n",
      "        [-0.3309,  0.4767, -0.1651,  ..., -0.4447,  0.5558, -0.1007]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [247, 448, 411,  ..., 499, 336,  13],\n",
      "        [150,  23, 169,  ..., 112, 180, 446],\n",
      "        ...,\n",
      "        [ 99, 304, 190,  ..., 190, 190, 190],\n",
      "        [429, 190, 190,  ..., 190, 190, 190],\n",
      "        [124, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[0, 1, 1,  ..., 2, 2, 0],\n",
      "        [0, 1, 1,  ..., 2, 2, 0],\n",
      "        [0, 1, 1,  ..., 2, 2, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 2, 2, 0],\n",
      "        [0, 1, 1,  ..., 2, 2, 0],\n",
      "        [0, 1, 1,  ..., 2, 2, 0]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.2978,  0.8217, -0.1066,  ..., -0.5862,  0.6313,  0.2233],\n",
      "        [-0.3132,  0.4786, -0.1600,  ..., -0.4415,  0.5450, -0.1095],\n",
      "        [-0.3132,  0.4786, -0.1600,  ..., -0.4415,  0.5450, -0.1095],\n",
      "        ...,\n",
      "        [-0.2558,  0.6516, -0.2000,  ..., -0.4766,  0.5379,  0.0427],\n",
      "        [-0.2558,  0.6516, -0.2000,  ..., -0.4766,  0.5379,  0.0427],\n",
      "        [-0.2978,  0.8217, -0.1066,  ..., -0.5862,  0.6313,  0.2233]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [454, 448, 448,  ..., 469,  87, 199],\n",
      "        [112,  99, 436,  ..., 429, 431, 124],\n",
      "        ...,\n",
      "        [321, 153, 190,  ..., 190, 190, 190],\n",
      "        [112, 190, 190,  ..., 190, 190, 190],\n",
      "        [ 71, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[0, 4, 0,  ..., 2, 2, 1],\n",
      "        [0, 4, 0,  ..., 2, 2, 1],\n",
      "        [0, 4, 0,  ..., 2, 2, 1],\n",
      "        ...,\n",
      "        [0, 4, 0,  ..., 2, 2, 1],\n",
      "        [0, 4, 0,  ..., 2, 2, 1],\n",
      "        [0, 4, 0,  ..., 2, 2, 1]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.2721,  0.8235, -0.1036,  ..., -0.5818,  0.6204,  0.2126],\n",
      "        [-0.3395,  0.5828, -0.0726,  ..., -0.5229,  0.6144, -0.0106],\n",
      "        [-0.2721,  0.8235, -0.1036,  ..., -0.5818,  0.6204,  0.2126],\n",
      "        ...,\n",
      "        [-0.2345,  0.6543, -0.1964,  ..., -0.4729,  0.5272,  0.0344],\n",
      "        [-0.2345,  0.6543, -0.1964,  ..., -0.4729,  0.5272,  0.0344],\n",
      "        [-0.2954,  0.4808, -0.1567,  ..., -0.4375,  0.5342, -0.1172]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [448, 448, 169,  ..., 139, 378, 455],\n",
      "        [  0, 422, 124,  ..., 180, 169, 150],\n",
      "        ...,\n",
      "        [ 99, 169, 190,  ..., 190, 190, 190],\n",
      "        [112, 187, 190,  ..., 190, 190, 190],\n",
      "        [ 19, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[1, 7, 2,  ..., 6, 2, 2],\n",
      "        [1, 7, 2,  ..., 6, 2, 2],\n",
      "        [1, 7, 2,  ..., 6, 2, 2],\n",
      "        ...,\n",
      "        [1, 7, 2,  ..., 6, 2, 2],\n",
      "        [1, 7, 2,  ..., 6, 2, 2],\n",
      "        [1, 7, 2,  ..., 6, 2, 2]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.2781,  0.4832, -0.1532,  ..., -0.4331,  0.5229, -0.1249],\n",
      "        [-0.4162,  0.5882,  0.0597,  ..., -0.5987,  0.7124,  0.0137],\n",
      "        [-0.2140,  0.6570, -0.1926,  ..., -0.4685,  0.5160,  0.0261],\n",
      "        ...,\n",
      "        [-0.0782,  0.4867, -0.4151,  ..., -0.2736,  0.3198, -0.1434],\n",
      "        [-0.2140,  0.6570, -0.1926,  ..., -0.4685,  0.5160,  0.0261],\n",
      "        [-0.2140,  0.6570, -0.1926,  ..., -0.4685,  0.5160,  0.0261]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [247, 448, 411,  ..., 373, 370, 139],\n",
      "        [150, 454, 446,  ..., 150,  26, 169],\n",
      "        ...,\n",
      "        [150, 190, 190,  ..., 190, 190, 190],\n",
      "        [177, 190, 190,  ..., 190, 190, 190],\n",
      "        [177, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[0, 1, 2,  ..., 1, 2, 2],\n",
      "        [0, 1, 2,  ..., 1, 2, 2],\n",
      "        [0, 1, 2,  ..., 1, 2, 2],\n",
      "        ...,\n",
      "        [0, 1, 2,  ..., 1, 2, 2],\n",
      "        [0, 1, 2,  ..., 1, 2, 2],\n",
      "        [0, 1, 2,  ..., 1, 2, 2]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[-0.2246,  0.8268, -0.0978,  ..., -0.5715,  0.5997,  0.1903],\n",
      "        [-0.2616,  0.4856, -0.1500,  ..., -0.4287,  0.5127, -0.1334],\n",
      "        [-0.1947,  0.6597, -0.1890,  ..., -0.4642,  0.5064,  0.0169],\n",
      "        ...,\n",
      "        [-0.2616,  0.4856, -0.1500,  ..., -0.4287,  0.5127, -0.1334],\n",
      "        [-0.1947,  0.6597, -0.1890,  ..., -0.4642,  0.5064,  0.0169],\n",
      "        [-0.1947,  0.6597, -0.1890,  ..., -0.4642,  0.5064,  0.0169]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n",
      "tensor([[ 30,  30,  30,  ...,  30,  30,  30],\n",
      "        [ 87, 448,  13,  ..., 139, 123, 139],\n",
      "        [169, 378, 357,  ...,   9, 436,   9],\n",
      "        ...,\n",
      "        [180, 303, 190,  ..., 190, 190, 190],\n",
      "        [303, 190, 190,  ..., 190, 190, 190],\n",
      "        [112, 190, 190,  ..., 190, 190, 190]])\n",
      "tensor([[3, 0, 1,  ..., 0, 1, 0],\n",
      "        [3, 0, 1,  ..., 0, 1, 0],\n",
      "        [3, 0, 1,  ..., 0, 1, 0],\n",
      "        ...,\n",
      "        [3, 0, 1,  ..., 0, 1, 0],\n",
      "        [3, 0, 1,  ..., 0, 1, 0],\n",
      "        [3, 0, 1,  ..., 0, 1, 0]])\n",
      "OUTPUTTTTTT\n",
      "tensor([[ 0.0085,  0.8019, -0.4149,  ..., -0.3815,  0.3656,  0.1152],\n",
      "        [-0.2019,  0.8302, -0.0951,  ..., -0.5660,  0.5900,  0.1802],\n",
      "        [-0.2451,  0.4887, -0.1465,  ..., -0.4240,  0.5023, -0.1409],\n",
      "        ...,\n",
      "        [-0.2019,  0.8302, -0.0951,  ..., -0.5660,  0.5900,  0.1802],\n",
      "        [-0.2451,  0.4887, -0.1465,  ..., -0.4240,  0.5023, -0.1409],\n",
      "        [-0.2019,  0.8302, -0.0951,  ..., -0.5660,  0.5900,  0.1802]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 510])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-522733ffd82c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UW/UW_Classes/21wi_EE511/ee511_assignments/assignment5/answer_key/key_tools.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainData, epoch, optimizer, PAD)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtrain_ppl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    if verbose: print(epoch+1)\n",
    "    train_ppl = tools.train(model, data_train, epoch, optimizer, PAD)\n",
    "    val_loss, val_ppl = tools.test(model, data_val, PAD)\n",
    "predictions = tools.get_predictions(model, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics(predictions, data_test.langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59\n",
      "torch.Size([146, 2304])\n",
      "[262, 262, 262, 262, 262, 262, 262, 262, 262, 448, 448, 448, 448, 448, 448, 448, 448, 448, 53, 53, 53, 53, 53, 53, 53, 53, 53, 187, 187, 187, 187, 187, 187, 187, 187, 187, 336, 336, 336, 336, 336, 336, 336, 336, 336, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 176, 176, 176, 176, 176, 176, 176, 176, 176, 448, 448, 448, 448, 448, 448, 448, 448, 448, 469, 469, 469, 469, 469, 469, 469, 469, 469, 455, 455, 455, 455, 455, 455, 455, 455, 455, 454, 454, 454, 454, 454, 454, 454, 454, 454, 73, 73, 73, 73, 73, 73, 73, 73, 73, 411, 411, 411, 411, 411, 411, 411, 411, 411, 448, 448, 448, 448, 448, 448, 448, 448, 448, 176, 176, 176, 176, 176, 176, 176, 176, 176, 247, 247, 247, 247, 247, 247, 247, 247, 247, 448, 448, 448, 448, 448, 448, 448, 448, 448, 449, 449, 449, 449, 449, 449, 449, 449, 449, 176, 176, 176, 176, 176, 176, 176, 176, 176, 495, 495, 495, 495, 495, 495, 495, 495, 495, 378, 378, 378, 378, 378, 378, 378, 378, 378, 411, 411, 411, 411, 411, 411, 411, 411, 411, 247, 247, 247, 247, 247, 247, 247, 247, 247, 0, 0, 0, 0, 0, 0, 0, 0, 0, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 411, 411, 411, 411, 411, 411, 411, 411, 411, 370, 370, 370, 370, 370, 370, 370, 370, 370, 312, 312, 312, 312, 312, 312, 312, 312, 312, 87, 87, 87, 87, 87, 87, 87, 87, 87, 376, 376, 376, 376, 376, 376, 376, 376, 376, 378, 378, 378, 378, 378, 378, 378, 378, 378, 176, 176, 176, 176, 176, 176, 176, 176, 176, 87, 87, 87, 87, 87, 87, 87, 87, 87, 448, 448, 448, 448, 448, 448, 448, 448, 448, 455, 455, 455, 455, 455, 455, 455, 455, 455, 411, 411, 411, 411, 411, 411, 411, 411, 411, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 247, 247, 247, 247, 247, 247, 247, 247, 247, 13, 13, 13, 13, 13, 13, 13, 13, 13, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 312, 312, 312, 312, 312, 312, 312, 312, 312, 448, 448, 448, 448, 448, 448, 448, 448, 448, 0, 0, 0, 0, 0, 0, 0, 0, 0, 448, 448, 448, 448, 448, 448, 448, 448, 448, 455, 455, 455, 455, 455, 455, 455, 455, 455, 373, 373, 373, 373, 373, 373, 373, 373, 373, 336, 336, 336, 336, 336, 336, 336, 336, 336, 448, 448, 448, 448, 448, 448, 448, 448, 448, 373, 373, 373, 373, 373, 373, 373, 373, 373, 448, 448, 448, 448, 448, 448, 448, 448, 448, 378, 378, 378, 378, 378, 378, 378, 378, 378, 297, 297, 297, 297, 297, 297, 297, 297, 297, 455, 455, 455, 455, 455, 455, 455, 455, 455, 448, 448, 448, 448, 448, 448, 448, 448, 448, 336, 336, 336, 336, 336, 336, 336, 336, 336, 247, 247, 247, 247, 247, 247, 247, 247, 247, 448, 448, 448, 448, 448, 448, 448, 448, 448, 378, 378, 378, 378, 378, 378, 378, 378, 378, 139, 139, 139, 139, 139, 139, 139, 139, 139, 336, 336, 336, 336, 336, 336, 336, 336, 336, 382, 382, 382, 382, 382, 382, 382, 382, 382, 448, 448, 448, 448, 448, 448, 448, 448, 448, 378, 378, 378, 378, 378, 378, 378, 378, 378, 71, 71, 71, 71, 71, 71, 71, 71, 71, 336, 336, 336, 336, 336, 336, 336, 336, 336, 1, 1, 1, 1, 1, 1, 1, 1, 1, 231, 231, 231, 231, 231, 231, 231, 231, 231, 247, 247, 247, 247, 247, 247, 247, 247, 247, 411, 411, 411, 411, 411, 411, 411, 411, 411, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 176, 176, 176, 176, 176, 176, 176, 176, 455, 455, 455, 455, 455, 455, 455, 455, 455, 0, 0, 0, 0, 0, 0, 0, 0, 0, 231, 231, 231, 231, 231, 231, 231, 231, 231, 278, 278, 278, 278, 278, 278, 278, 278, 278, 336, 336, 336, 336, 336, 336, 336, 336, 336, 448, 448, 448, 448, 448, 448, 448, 448, 448, 187, 187, 187, 187, 187, 187, 187, 187, 187, 455, 455, 455, 455, 455, 455, 455, 455, 455, 357, 357, 357, 357, 357, 357, 357, 357, 357, 469, 469, 469, 469, 469, 469, 469, 469, 469, 139, 139, 139, 139, 139, 139, 139, 139, 139, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 448, 448, 448, 448, 448, 448, 448, 448, 448, 318, 318, 318, 318, 318, 318, 318, 318, 318, 336, 336, 336, 336, 336, 336, 336, 336, 336, 53, 53, 53, 53, 53, 53, 53, 53, 53, 448, 448, 448, 448, 448, 448, 448, 448, 448, 139, 139, 139, 139, 139, 139, 139, 139, 139, 312, 312, 312, 312, 312, 312, 312, 312, 312, 378, 378, 378, 378, 378, 378, 378, 378, 378, 373, 373, 373, 373, 373, 373, 373, 373, 373, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 455, 455, 455, 455, 455, 455, 455, 455, 455, 73, 73, 73, 73, 73, 73, 73, 73, 73, 247, 247, 247, 247, 247, 247, 247, 247, 247, 174, 174, 174, 174, 174, 174, 174, 174, 174, 87, 87, 87, 87, 87, 87, 87, 87, 87, 370, 370, 370, 370, 370, 370, 370, 370, 370, 448, 448, 448, 448, 448, 448, 448, 448, 448, 312, 312, 312, 312, 312, 312, 312, 312, 312, 448, 448, 448, 448, 448, 448, 448, 448, 448, 247, 247, 247, 247, 247, 247, 247, 247, 247, 373, 373, 373, 373, 373, 373, 373, 373, 373, 247, 247, 247, 247, 247, 247, 247, 247, 247, 71, 71, 71, 71, 71, 71, 71, 71, 71, 448, 448, 448, 448, 448, 448, 448, 448, 448, 455, 455, 455, 455, 455, 455, 455, 455, 455, 13, 13, 13, 13, 13, 13, 13, 13, 13, 259, 259, 259, 259, 259, 259, 259, 259, 259, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 73, 73, 73, 73, 73, 73, 73, 73, 73, 411, 411, 411, 411, 411, 411, 411, 411, 411, 469, 469, 469, 469, 469, 469, 469, 469, 469, 411, 411, 411, 411, 411, 411, 411, 411, 411, 231, 231, 231, 231, 231, 231, 231, 231, 231, 247, 247, 247, 247, 247, 247, 247, 247, 247, 87, 87, 87, 87, 87, 87, 87, 87, 87, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 431, 431, 431, 431, 431, 431, 431, 431, 431, 373, 373, 373, 373, 373, 373, 373, 373, 373, 448, 448, 448, 448, 448, 448, 448, 448, 448, 378, 378, 378, 378, 378, 378, 378, 378, 378, 139, 139, 139, 139, 139, 139, 139, 139, 139, 449, 449, 449, 449, 449, 449, 449, 449, 449, 373, 373, 373, 373, 373, 373, 373, 373, 373, 278, 278, 278, 278, 278, 278, 278, 278, 278, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 247, 247, 247, 247, 247, 247, 247, 247, 247, 455, 455, 455, 455, 455, 455, 455, 455, 455, 26, 26, 26, 26, 26, 26, 26, 26, 26, 469, 469, 469, 469, 469, 469, 469, 469, 469, 436, 436, 436, 436, 436, 436, 436, 436, 436, 370, 370, 370, 370, 370, 370, 370, 370, 370, 99, 99, 99, 99, 99, 99, 99, 99, 99, 321, 321, 321, 321, 321, 321, 321, 321, 321, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 373, 373, 373, 373, 373, 373, 373, 373, 373, 411, 411, 411, 411, 411, 411, 411, 411, 411, 448, 448, 448, 448, 448, 448, 448, 448, 448, 247, 247, 247, 247, 247, 247, 247, 247, 247, 455, 455, 455, 455, 455, 455, 455, 455, 455, 336, 336, 336, 336, 336, 336, 336, 336, 336, 247, 247, 247, 247, 247, 247, 247, 247, 247, 448, 448, 448, 448, 448, 448, 448, 448, 448, 297, 297, 297, 297, 297, 297, 297, 297, 297, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 449, 449, 449, 449, 449, 449, 449, 449, 449, 201, 201, 201, 201, 201, 201, 201, 201, 201, 318, 318, 318, 318, 318, 318, 318, 318, 318, 336, 336, 336, 336, 336, 336, 336, 336, 336, 176, 176, 176, 176, 176, 176, 176, 176, 176, 259, 259, 259, 259, 259, 259, 259, 259, 259, 73, 73, 73, 73, 73, 73, 73, 73, 73, 448, 448, 448, 448, 448, 448, 448, 448, 448, 234, 234, 234, 234, 234, 234, 234, 234, 234, 231, 231, 231, 231, 231, 231, 231, 231, 231, 455, 455, 455, 455, 455, 455, 455, 455, 455, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 455, 455, 455, 455, 455, 455, 455, 455, 455, 169, 169, 169, 169, 169, 169, 169, 169, 169, 139, 139, 139, 139, 139, 139, 139, 139, 139, 448, 448, 448, 448, 448, 448, 448, 448, 448, 0, 0, 0, 0, 0, 0, 0, 0, 0, 448, 448, 448, 448, 448, 448, 448, 448, 448, 71, 71, 71, 71, 71, 71, 71, 71, 71, 87, 87, 87, 87, 87, 87, 87, 87, 87, 400, 400, 400, 400, 400, 400, 400, 400, 400, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 373, 373, 373, 373, 373, 373, 373, 373, 373, 231, 231, 231, 231, 231, 231, 231, 231, 231, 321, 321, 321, 321, 321, 321, 321, 321, 321, 139, 139, 139, 139, 139, 139, 139, 139, 139, 378, 378, 378, 378, 378, 378, 378, 378, 378, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 13, 13, 13, 13, 13, 13, 13, 13, 13, 448, 448, 448, 448, 448, 448, 448, 448, 448, 176, 176, 176, 176, 176, 176, 176, 176, 176, 448, 448, 448, 448, 448, 448, 448, 448, 448, 336, 336, 336, 336, 336, 336, 336, 336, 336, 231, 231, 231, 231, 231, 231, 231, 231, 231, 448, 448, 448, 448, 448, 448, 448, 448, 448, 373, 373, 373, 373, 373, 373, 373, 373, 373, 436, 436, 436, 436, 436, 436, 436, 436, 436, 229, 229, 229, 229, 229, 229, 229, 229, 229, 336, 336, 336, 336, 336, 336, 336, 336, 336, 449, 449, 449, 449, 449, 449, 449, 449, 449, 87, 87, 87, 87, 87, 87, 87, 87, 87, 411, 411, 411, 411, 411, 411, 411, 411, 411, 454, 454, 454, 454, 454, 454, 454, 454, 454, 448, 448, 448, 448, 448, 448, 448, 448, 448, 0, 0, 0, 0, 0, 0, 0, 0, 0, 448, 448, 448, 448, 448, 448, 448, 448, 448, 87, 87, 87, 87, 87, 87, 87, 87, 87, 448, 448, 448, 448, 448, 448, 448, 448, 448, 436, 436, 436, 436, 436, 436, 436, 436, 436, 53, 53, 53, 53, 53, 53, 53, 53, 53, 297, 297, 297, 297, 297, 297, 297, 297, 297, 231, 231, 231, 231, 231, 231, 231, 231, 231, 169, 169, 169, 169, 169, 169, 169, 169, 169, 499, 499, 499, 499, 499, 499, 499, 499, 499, 449, 449, 449, 449, 449, 449, 449, 449, 449, 231, 231, 231, 231, 231, 231, 231, 231, 231, 278, 278, 278, 278, 278, 278, 278, 278, 278, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 259, 259, 259, 259, 259, 259, 259, 259, 259, 373, 373, 373, 373, 373, 373, 373, 373, 373, 242, 242, 242, 242, 242, 242, 242, 242, 242, 336, 336, 336, 336, 336, 336, 336, 336, 336, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 370, 370, 370, 370, 370, 370, 370, 370, 370, 448, 448, 448, 448, 448, 448, 448, 448, 448, 455, 455, 455, 455, 455, 455, 455, 455, 455, 318, 318, 318, 318, 318, 318, 318, 318, 318, 112, 112, 112, 112, 112, 112, 112, 112, 112, 448, 448, 448, 448, 448, 448, 448, 448, 448, 370, 370, 370, 370, 370, 370, 370, 370, 370, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 312, 312, 312, 312, 312, 312, 312, 312, 312, 455, 455, 455, 455, 455, 455, 455, 455, 455, 373, 373, 373, 373, 373, 373, 373, 373, 373, 357, 357, 357, 357, 357, 357, 357, 357, 357, 466, 466, 466, 466, 466, 466, 466, 466, 466, 499, 499, 499, 499, 499, 499, 499, 499, 499, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 448, 378, 378, 378, 378, 378, 378, 378, 378, 378, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 336, 336, 336, 336, 336, 336, 336, 336, 336, 124, 124, 124, 124, 124, 124, 124, 124, 124]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "torch.Size([146, 9, 510])\n"
     ]
    }
   ],
   "source": [
    "predictions = tools.get_predictions(model, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}