{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from metrics import Metrics\n",
    "# Some magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lan_codes = ['en', 'es', 'pt', 'gl', 'eu', 'ca', 'fr', 'it', 'de']\n",
    "Languages = ['English', 'Spanish', 'Portuguese', 'Galician', 'Basque', 'Catalan', 'French', 'Italian', 'German']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data, Calculate Vocabulary and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of the vocabulary: 509 characters\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "data = tools.Data()\n",
    "print('Size of the vocabulary: %d characters' % len(data.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Perplexity measurement is 34.11\n"
     ]
    }
   ],
   "source": [
    "preplexity = data.get_perplexity()\n",
    "print('Perplexity measurement is %.2f' % preplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Percent of Invalid Characters - Train: 0.04622%\nPercent of Invalid Characters - Val: 0.05987%\n"
     ]
    }
   ],
   "source": [
    "print('Percent of Invalid Characters - Train: %.5f%%' \n",
    "    % ((data.train_freq[data.vocab.index('<N>')] / data.train_freq.sum()) * 100.0))\n",
    "print('Percent of Invalid Characters - Val: %.5f%%' \n",
    "    % ((data.val_freq[data.vocab.index('<N>')] / data.val_freq.sum()) * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell takes a few minutes to run\n",
    "train_tweets, train_lans = tools.data_encoding(data.train, data.vocab, lan_codes)\n",
    "val_tweets, val_lans = tools.data_encoding(data.val, data.vocab, lan_codes)\n",
    "test_tweets, test_lans = tools.data_encoding(data.test, data.vocab, lan_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, lang_size=9, char_vec_size=12, lang_vec_size=2, hidden_size=50, PAD=0):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lang_size = lang_size\n",
    "        self.char_vec_size = char_vec_size\n",
    "        self.lang_vec_size = lang_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.char_encoder = nn.Embedding(self.vocab_size, self.char_vec_size)\n",
    "        self.lang_encoder = nn.Embedding(self.lang_size, self.lang_vec_size)\n",
    "        # the current hidden size = char_vec_size\n",
    "        self.gru = nn.GRU(self.char_vec_size+self.lang_vec_size, self.hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.char_vec_size)\n",
    "        self.decoder = nn.Linear(self.char_vec_size, self.vocab_size)\n",
    "        \n",
    "        # This shares the encoder and decoder weights as described in lecture.\n",
    "        self.decoder.weight = self.char_encoder.weight\n",
    "        self.decoder.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "        weight = torch.ones(vocab_size)\n",
    "        # scores over PAD is not counted\n",
    "        weight[PAD] = 0\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "        self.crit = nn.NLLLoss(weight, size_average=False)\n",
    "\n",
    "    def forward(self, tweets, lang, hidden=None):\n",
    "        emb = torch.cat((self.lang_encoder(lang), self.char_encoder(tweets)), -1)\n",
    "        output, hidden_t = self.gru(emb, hidden)\n",
    "        output = F.tanh(self.linear(output))\n",
    "        output = self.decoder(output)\n",
    "        return output, hidden_t\n",
    "\n",
    "    # Predefined loss function\n",
    "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
    "        prediction = prediction.view(-1, self.vocab_size)\n",
    "        prediction = self.sm(prediction)\n",
    "        loss_val = self.crit(prediction, label.view(-1))\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "FEATURE_SIZE = 15\n",
    "TEST_BATCH_SIZE = 256\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "MOMENTUM = 0.5\n",
    "LOG_INTERVAL = 500\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(len(data.vocab), PAD=data.vocab.index('</S>'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 0 [0/80175 (0%)]\tLoss: 75423.132812\n",
      "Train Epoch: 0 [32000/80175 (40%)]\tLoss: 1229.884888\n",
      "Train Epoch: 0 [64000/80175 (80%)]\tLoss: 651.479797\n",
      "Train Epoch: 1 [0/80175 (0%)]\tLoss: 325.552063\n",
      "Train Epoch: 1 [32000/80175 (40%)]\tLoss: 160.575851\n",
      "Train Epoch: 1 [64000/80175 (80%)]\tLoss: 173.832031\n",
      "Train Epoch: 2 [0/80175 (0%)]\tLoss: 66.720955\n",
      "Train Epoch: 2 [32000/80175 (40%)]\tLoss: 36.463741\n",
      "Train Epoch: 2 [64000/80175 (80%)]\tLoss: 29.066631\n"
     ]
    }
   ],
   "source": [
    "tools.train(model, device, train_loader, optimizer, EPOCHS, LOG_INTERVAL,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Perplexity : 184.42515049270366\nLoss       : 5314.205371856689\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5314.205371856689, 184.42515049270366)"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "tools.test(model, device, val_loader, data.vocab.index('</S>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = torch.tensor(test_tweets, dtype=torch.long, device=torch.device(\"cpu\"))\n",
    "language_val = torch.tensor(test_lans, dtype=torch.long, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tools.predict(model, device, tweets_test[:5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Percent Correct: 30.72\n"
     ]
    }
   ],
   "source": [
    "print(f'Percent Correct: {np.sum(pred == test_lans[:5000,0])/pred.shape[0]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy = 0.307\n Lang     Prec.   Rec.   F1\n------------------------------\n     0.0   31.70  97.02  47.79\n     1.0    0.00   0.00   0.00\n     2.0    0.00   0.00   0.00\n     3.0    0.00   0.00   0.00\n     4.0    0.00   0.00   0.00\n     5.0    0.00   0.00   0.00\n     6.0    4.44   3.54   3.94\n     7.0    0.00   0.00   0.00\n     8.0    0.00   0.00   0.00\n     9.0    0.00   0.00   0.00\n    10.0    0.00   0.00   0.00\n    11.0    0.00   0.00   0.00\n    12.0    0.00   0.00   0.00\n    13.0    0.00   0.00   0.00\n    14.0    0.00   0.00   0.00\n------------------------------\n  Total:    2.41   6.70   3.45\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.448548036938752"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "Metrics(pred, test_lans[:5000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}