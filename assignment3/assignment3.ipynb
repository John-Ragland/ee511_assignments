{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tools import Data\n",
    "from collections import Counter\n",
    "\n",
    "# Some magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this step takes a while because:\n",
    "#   it first calculates vocab\n",
    "#   then processes test and training data (creates a vector of only the words that are in the vocab)\n",
    "# nice thing is, you only need to run this once and all your data is processed\n",
    "# might be worth trying to improve efficieny later on\n",
    "\n",
    "# data.vocab => returns list of 5000 most frequent words in training data\n",
    "# data.categories => returns list of the 20 category names\n",
    "# data.test_data and data.train_data => returns a list of 20 arrays (each array refers to a category), each category contains an array of post vectors\n",
    "# data,test_data[a][b][c] => a: category, b: vector of a post in that category, c: frequency of the word self.vocab[c] in this post\n",
    "\n",
    "# in categories, test_data and train_data indicies refer to the same catery. \n",
    "# Ex: 0th index is alt.atheism category on all 3 lists\n",
    "\n",
    "data = Data('data/20ng-test-all-terms.txt', 'data/20ng-train-all-terms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14\nthe\n"
     ]
    }
   ],
   "source": [
    "# if you go to the test file and look at the first line, there are actually 14 \"the\"s\n",
    "print(data.test_data[0][0][0])\n",
    "print(data.vocab[0])"
   ]
  },
  {
   "source": [
    "# Note to John:\n",
    "I think steps a, b and c are good to go (unless we decide to implement some other way)<br>\n",
    "For b, I just used the top 5000 words. See comments in the above cell to how to use the \"Data\" object.<br>\n",
    "For c, I picked method two since it seemed the simplest to implement. I set each post vector to be 5000 long (as long as the vocab) and if a vocab word never existed in the, i put 0 in its place. The instructions aren't clear about this. This approach made the most sense to me (so that our vectors have consistent dimensions). Let me know what you think. <br>\n",
    "\n",
    "Annnd feel free to delete these comments after you read them.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}